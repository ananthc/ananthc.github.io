---
published: true
publish: true
comments: true
tags:
  - Apex
  - Kudu
categories:
  - Misc
permalink: High throughput low latency streaming using Apache Apex and Apache Kudu
gallery:
  - url: >-
      /assets/images/high-throughput-low-latency-streaming-using-kudu-apex/Sample-Kudu-Output-DAG-Application.png
    image_path: >-
      /assets/images/high-throughput-low-latency-streaming-using-kudu-apex/Sample-Kudu-Output-DAG-Application.png
    alt: Sample Apex DAG for Kudu output operator
---
## Introduction


The last few years has seen HDFS as a great enabler that would help organizations store extremely large amounts of data on commodity hardware. However over the last couple of years the technology landscape changed rapidly and new age engines like Apache Spark, Apache Apex and Apache Flink have started enabling more powerful use cases on a distributed data store paradigm. This has quickly brought out the short-comings of an immutable data store. The primary short comings are
- Immutability resulted in complex lambda architectures when HDFS is used as a store by a query engine
- When data files had to be generated in time bound windows data pipeline frameworks resulted in creating files which are very small in size. Over a period of time this resulted in very small sized files in very large numbers eating up the namenode namespaces to a very great extent.
- With the arrival of SQL-on-Hadoop in a big way and the introduction new age SQL engines like Impala, ETL pipelines resulted in choosing columnar oriented formats albeit with a penalty of accumulating data for a while to gain advantages of the columnar format storage on disk. This reduced the impact of "information now" approach for a hadoop eco system based solution.

Apache Kudu is a next generation storage engine that comes with the following strong points 
- A distributed store
- No single point of failure by adopting the RAFT consensus algorithm under the hood
- Mutable 
- Auto compaction of data sets
- Columnar storage model wrapped over a simple CRUD style API
- Tabular structure as a storage model
- Bulk scan patterns possible 
- Predicate push downs when scanning

This post explores the capabilties of [Apache Kudu](https://kudu.apache.org/) in conjunction with the Apex streaming engine. [Apache Apex](https://apex.apache.org/) is a low latency streaming engine which can run on top of YARN and provides many Enterprise grade features out of the box. The post describes the features using a hypothetical use case. The transactions that are processed by a streaming engine need to be written to a data store and subsequently avaiable for a read pattern. The caveat is that the write path needs to be completed in sub-second time windows and read paths should be available within sub-second time frames once the data is written.

## Apex integration
Apache Apex integration with Apache Kudu is released as part of the [Apache Malhar](https://apex.apache.org/docs/malhar/) library. Apache Malhar is a library of operators that are compatible with Apache Apex. This integration is available from the 3.8.0 release of Apache Malhar library. 

Apache Apex integration with Apache Kudu is implemented for two major use cases. An apex Operator ( A JVM instance that makes up the Streaming DAG application ) is a logical unit that provides a specific piece of functionality. In the case of Kudu integration, Apex provided for two types of operators  

- A write path is implemented by the Kudu Output operator 
- A read path is implemented by the Kudu Input Operator.

Apex uses the 1.5.0 version of the java client driver of Kudu.



## Write paths

In our example, transactions( rows of data)  are processed by Apex engine for fraud. As soon as the fraud score is generated by the Apex engine, the row needs to be persisted into a Kudu table. The following are the main features supported by the Apache Apex integration with Apache Kudu.

### Write modes at a tuple level
The kudu outout operator allows for writes to happen to be defined at a tuple level. The following modes are supported of every tuple that is written to a Kudu table by the Apex engine.
- Insert 
- Update
- Upsert
- Delete

![Apex DAG for Kudu output operator]({{site.baseurl}}/assets/images/high-throughput-low-latency-streaming-using-kudu-apex/Sample-Kudu-Output-DAG-Application.png)


### Automatic mapping of POJO column names to Kudu table column names
Kudu output operator uses the Kudu java driver to obtain the metadata of the Kudu table. By using the metadata API, Kudu output operator allows for automatic mapping of a POJO field name to the Kudu table column name. Of course this mapping can be manually overridden when creating a new instance of the Kudu output operator in the Apex application

### Selective column writes
Kudu output operator also allows for only writing a subset of columns for a given Kudu table row. This optimization allows for writing select columns without performing a read of the current column thus allowing for higher throughput for writes.

For example, in the device info table as part of the fraud processing application, we could choose to write only the "last seen" column and avoid a read of the entire row.
![Selective column writes]({{site.baseurl}}/assets/images/high-throughput-low-latency-streaming-using-kudu-apex/Kudu-Output-Operator-selective-col-writes.png)




## Read paths

