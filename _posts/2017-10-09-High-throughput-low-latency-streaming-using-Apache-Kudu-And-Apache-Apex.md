---
published: true
publish: true
comments: true
tags:
  - Apex
  - Kudu
categories:
  - Misc
permalink: High throughput low latency streaming using Apache Apex and Apache Kudu
---
## Introduction


The last few years has seen HDFS as a great enabler that would help organizations store extremely large amounts of data on commodity hardware. However over the last couple of years the technology landscape changed rapidly and new age engines like Apache Spark, Apache Apex and Apache Flink have started enabling more powerful use cases on a distributed data store paradigm. This has quickly brought out the short-comings of an immutable data store. The primary short comings are
- Immutability resulted in complex lambda architectures when HDFS is used as a store by a query engine
- When data files had to be generated in time bound windows data pipeline frameworks resulted in creating files which are very small in size. Over a period of time this resulted in very small sized files in very large numbers eating up the namenode namespaces to a very great extent.
- With the arrival of SQL-on-Hadoop in a big way and the introduction new age SQL engines like Impala, ETL pipelines resulted in choosing columnar oriented formats albeit with a penalty of accumulating data for a while to gain advantages of the columnar format storage on disk. This reduced the impact of "information now" approach for a hadoop eco system based solution.

Apache Kudu is a next generation storage engine that comes with the following strong points 
- A distributed store
- No single point of failure by adopting the RAFT consensus algorithm under the hood
- Mutable 
- Auto compaction of data sets
- Columnar storage model wrapped over a simple CRUD style API
- Tabular structure as a storage model
- Bulk scan patterns possible 
- Predicate push downs when scanning

This post explores the capabilties of [Apache Kudu](https://kudu.apache.org/) in conjunction with the Apex streaming engine. [Apache Apex](https://apex.apache.org/) is a low latency streaming engine which can run on top of YARN and provides many Enterprise grade features out of the box. The post describes the features using a hypothetical use case. The transactions that are processed by a streaming engine need to be written to a data store and subsequently avaiable for a read pattern. The caveat is that the write path needs to be completed in sub-second time windows and read paths should be available within sub-second time frames once the data is written.

## Apex integration
Apache Apex integration with Apache Kudu is released as part of the [Apache Malhar](https://apex.apache.org/docs/malhar/) library. Apache Malhar is a library of operators that are compatible with Apache Apex. This integration is available from the 3.8.0 release of Apache Malhar library. 

Apache Apex integration with Apache Kudu is implemented for two major use cases. An apex Operator ( A JVM instance that makes up the Streaming DAG application ) is a logical unit that provides a specific piece of functionality. In the case of Kudu integration, Apex provided for two types of operators  

- A write path is implemented by the Kudu Output operator 
- A read path is implemented by the Kudu Input Operator.

## Write paths

In our example, transactions( rows of data)  are processed by Apex engine for fraud. As soon as the fraud score is generated by the Apex engine, the row needs to be persisted into a Kudu table. The following are the main features supported by the Apache Apex integration with Apache Kudu.

### Write modes at a tuple level

## Read paths

